---
# See: https://github.com/sigoden/aichat/blob/main/config.example.yaml

# ================================================
# LLM CONFIGURATION
# ================================================
model: github:gpt-4o
temperature: 0.2

# ================================================
# BEHAVIOR CONFIGURATION
# ================================================
save: false
keybindings: vi
wrap: auto
wrap_code: false

# ================================================
# FUNCTION CALLING CONFIGURATION
# ================================================
# Visit https://github.com/sigoden/llm-functions for setup instructions
function_calling: true
mapping_tools:
  fs: "fs_cat,fs_ls,fs_mkdir,fs_rm,fs_write"
use_tools: null

# ================================================
# PRELUDE CONFIGURATION
# ================================================
prelude: null
repl_prelude: null
agent_prelude: null

# ================================================
# SESSION CONFIGURATION
# ================================================
# Controls the persistence of the session. if true, auto save; if false, not save; if null, asking the user
save_session: false
# Compress session when token count reaches or exceeds this threshold
compress_threshold: 16000
# Text prompt used for creating a concise summary of session message
summarize_prompt: "Summarize the discussion briefly in 200 words or less to use as a prompt for future context."
# Text prompt used for including the summary of the entire session
summary_prompt: "This is a summary of the chat history as a recap: "

# ================================================
# RAG CONFIGURATION
# ================================================
# See [RAG-Guide](https://github.com/sigoden/aichat/wiki/RAG-Guide) for more details.
rag_embedding_model: nomic-embed-text:latest
rag_reranker_model: null
rag_chunk_size: 1500
rag_chunk_overlap: 75
# Define document loaders to control how RAG and `.file`/`--file` load files of specific formats.
document_loaders:
  docx: "pandoc --to plain $1"

# ================================================
# APPEARANCE CONFIGURATION
# ================================================
highlight: true
light_theme: false
# Custom REPL left/right prompts, see https://github.com/sigoden/aichat/wiki/Custom-REPL-Prompt for more details
left_prompt: "{?agent {color.purple}{agent}}{!agent {color.green}{role}}{?rag {color.yellow}@{rag}}{color.reset}) "
right_prompt: "{?consume_tokens {color.blue}{consume_tokens}({consume_percent}%)}{!consume_tokens {color.blue}{consume_tokens}} {?session {color.cyan}({session})}{color.reset}"

# ================================================
# CLIENTS CONFIGURATION
# ================================================
clients:
  # -------------------
  # GITHUB
  # -------------------
  - type: openai-compatible
    name: github
    api_base: https://models.inference.ai.azure.com
  # -------------------
  # LOCAL
  # -------------------
  - type: ollama
    api_base: http://127.0.0.1:11434/v1
    models:
      - name: nomic-embed-text
        type: embedding
        default_chunk_size: 1000
        max_batch_size: 50
      - name: llama3.1:8b
        max_input_tokens: 128000
        supports_function_calling: true
